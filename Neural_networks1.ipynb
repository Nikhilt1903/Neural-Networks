{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1nzTw4UrafS"
   },
   "source": [
    "#Data Exploration and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FduG2LPmlixS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YnwZ00X0q4qH"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0frpjwFPljPh"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df= pd.read_csv(\"Alphabets_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "IgZMDblOljMS",
    "outputId": "6a76d614-52f9-4969-ee17-2f83a78e8aed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letter</th>\n",
       "      <th>xbox</th>\n",
       "      <th>ybox</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>onpix</th>\n",
       "      <th>xbar</th>\n",
       "      <th>ybar</th>\n",
       "      <th>x2bar</th>\n",
       "      <th>y2bar</th>\n",
       "      <th>xybar</th>\n",
       "      <th>x2ybar</th>\n",
       "      <th>xy2bar</th>\n",
       "      <th>xedge</th>\n",
       "      <th>xedgey</th>\n",
       "      <th>yedge</th>\n",
       "      <th>yedgex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  letter  xbox  ybox  width  height  onpix  xbar  ybar  x2bar  y2bar  xybar  \\\n",
       "0      T     2     8      3       5      1     8    13      0      6      6   \n",
       "1      I     5    12      3       7      2    10     5      5      4     13   \n",
       "2      D     4    11      6       8      6    10     6      2      6     10   \n",
       "3      N     7    11      6       6      3     5     9      4      6      4   \n",
       "4      G     2     1      3       1      1     8     6      6      6      6   \n",
       "\n",
       "   x2ybar  xy2bar  xedge  xedgey  yedge  yedgex  \n",
       "0      10       8      0       8      0       8  \n",
       "1       3       9      2       8      4      10  \n",
       "2       3       7      3       7      3       9  \n",
       "3       4      10      6      10      2       8  \n",
       "4       5       9      1       7      5      10  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIIltuY3ljJT",
    "outputId": "6a5182ef-0ece-4cdd-e849-47778276c2e3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DMY1IhW1li4U",
    "outputId": "948d90c4-cf95-4b94-bcfa-0ba55f20bc1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 17 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   letter  20000 non-null  object\n",
      " 1   xbox    20000 non-null  int64 \n",
      " 2   ybox    20000 non-null  int64 \n",
      " 3   width   20000 non-null  int64 \n",
      " 4   height  20000 non-null  int64 \n",
      " 5   onpix   20000 non-null  int64 \n",
      " 6   xbar    20000 non-null  int64 \n",
      " 7   ybar    20000 non-null  int64 \n",
      " 8   x2bar   20000 non-null  int64 \n",
      " 9   y2bar   20000 non-null  int64 \n",
      " 10  xybar   20000 non-null  int64 \n",
      " 11  x2ybar  20000 non-null  int64 \n",
      " 12  xy2bar  20000 non-null  int64 \n",
      " 13  xedge   20000 non-null  int64 \n",
      " 14  xedgey  20000 non-null  int64 \n",
      " 15  yedge   20000 non-null  int64 \n",
      " 16  yedgex  20000 non-null  int64 \n",
      "dtypes: int64(16), object(1)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "id": "2kYyFBHAmjZz",
    "outputId": "cdf0246f-add4-4145-fe63-75688bfc2754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics of numerical features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xbox</th>\n",
       "      <th>ybox</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>onpix</th>\n",
       "      <th>xbar</th>\n",
       "      <th>ybar</th>\n",
       "      <th>x2bar</th>\n",
       "      <th>y2bar</th>\n",
       "      <th>xybar</th>\n",
       "      <th>x2ybar</th>\n",
       "      <th>xy2bar</th>\n",
       "      <th>xedge</th>\n",
       "      <th>xedgey</th>\n",
       "      <th>yedge</th>\n",
       "      <th>yedgex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.00000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.00000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.023550</td>\n",
       "      <td>7.035500</td>\n",
       "      <td>5.121850</td>\n",
       "      <td>5.37245</td>\n",
       "      <td>3.505850</td>\n",
       "      <td>6.897600</td>\n",
       "      <td>7.500450</td>\n",
       "      <td>4.628600</td>\n",
       "      <td>5.178650</td>\n",
       "      <td>8.282050</td>\n",
       "      <td>6.45400</td>\n",
       "      <td>7.929000</td>\n",
       "      <td>3.046100</td>\n",
       "      <td>8.338850</td>\n",
       "      <td>3.691750</td>\n",
       "      <td>7.80120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.913212</td>\n",
       "      <td>3.304555</td>\n",
       "      <td>2.014573</td>\n",
       "      <td>2.26139</td>\n",
       "      <td>2.190458</td>\n",
       "      <td>2.026035</td>\n",
       "      <td>2.325354</td>\n",
       "      <td>2.699968</td>\n",
       "      <td>2.380823</td>\n",
       "      <td>2.488475</td>\n",
       "      <td>2.63107</td>\n",
       "      <td>2.080619</td>\n",
       "      <td>2.332541</td>\n",
       "      <td>1.546722</td>\n",
       "      <td>2.567073</td>\n",
       "      <td>1.61747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               xbox          ybox         width       height         onpix  \\\n",
       "count  20000.000000  20000.000000  20000.000000  20000.00000  20000.000000   \n",
       "mean       4.023550      7.035500      5.121850      5.37245      3.505850   \n",
       "std        1.913212      3.304555      2.014573      2.26139      2.190458   \n",
       "min        0.000000      0.000000      0.000000      0.00000      0.000000   \n",
       "25%        3.000000      5.000000      4.000000      4.00000      2.000000   \n",
       "50%        4.000000      7.000000      5.000000      6.00000      3.000000   \n",
       "75%        5.000000      9.000000      6.000000      7.00000      5.000000   \n",
       "max       15.000000     15.000000     15.000000     15.00000     15.000000   \n",
       "\n",
       "               xbar          ybar         x2bar         y2bar         xybar  \\\n",
       "count  20000.000000  20000.000000  20000.000000  20000.000000  20000.000000   \n",
       "mean       6.897600      7.500450      4.628600      5.178650      8.282050   \n",
       "std        2.026035      2.325354      2.699968      2.380823      2.488475   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        6.000000      6.000000      3.000000      4.000000      7.000000   \n",
       "50%        7.000000      7.000000      4.000000      5.000000      8.000000   \n",
       "75%        8.000000      9.000000      6.000000      7.000000     10.000000   \n",
       "max       15.000000     15.000000     15.000000     15.000000     15.000000   \n",
       "\n",
       "            x2ybar        xy2bar         xedge        xedgey         yedge  \\\n",
       "count  20000.00000  20000.000000  20000.000000  20000.000000  20000.000000   \n",
       "mean       6.45400      7.929000      3.046100      8.338850      3.691750   \n",
       "std        2.63107      2.080619      2.332541      1.546722      2.567073   \n",
       "min        0.00000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        5.00000      7.000000      1.000000      8.000000      2.000000   \n",
       "50%        6.00000      8.000000      3.000000      8.000000      3.000000   \n",
       "75%        8.00000      9.000000      4.000000      9.000000      5.000000   \n",
       "max       15.00000     15.000000     15.000000     15.000000     15.000000   \n",
       "\n",
       "            yedgex  \n",
       "count  20000.00000  \n",
       "mean       7.80120  \n",
       "std        1.61747  \n",
       "min        0.00000  \n",
       "25%        7.00000  \n",
       "50%        8.00000  \n",
       "75%        9.00000  \n",
       "max       15.00000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics\n",
    "print(\"Summary statistics of numerical features:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "5GyhTOOymzEi",
    "outputId": "f240c1e4-0652-4aad-8c1d-f6b8b3918a00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZY1CG4-my8y",
    "outputId": "26fb8ac2-106c-4996-c70a-6f383ce4c9d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " letter    0\n",
      "xbox      0\n",
      "ybox      0\n",
      "width     0\n",
      "height    0\n",
      "onpix     0\n",
      "xbar      0\n",
      "ybar      0\n",
      "x2bar     0\n",
      "y2bar     0\n",
      "xybar     0\n",
      "x2ybar    0\n",
      "xy2bar    0\n",
      "xedge     0\n",
      "xedgey    0\n",
      "yedge     0\n",
      "yedgex    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bKbWYGwhmy5q",
    "outputId": "63de6e78-c2c8-4cc8-aa85-7a773b427de3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1332)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C4BfNRDomy2a",
    "outputId": "27649db6-b4e7-4fc9-9dc8-b3f72d3385fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after removal: 0\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Confirm the removal of duplicate rows\n",
    "print(\"after removal:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9vpRCHQpS1v"
   },
   "source": [
    "Normalization or Standardization: Normalize or standardize the numerical features to ensure they are on similar scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ips1QKRFpAOW"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['letter'])\n",
    "y = df['letter']\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPadHNoHpTgR"
   },
   "source": [
    "Handling Outliers: If necessary, handle outliers using techniques such as winsorization or robust scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ePJgtGHqpIaU"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Apply robust scaling to handle outliers\n",
    "robust_scaler = RobustScaler()\n",
    "X_scaled_robust = robust_scaler.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "PAqUi7Q2pIWp"
   },
   "outputs": [],
   "source": [
    "# Example of feature engineering (we can modify this based on your domain knowledge)\n",
    "X_engineered = pd.DataFrame(X_scaled_robust, columns=X.columns)\n",
    "X_engineered['area'] = X['width'] * X['height']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HE_l6I5nqwFi"
   },
   "source": [
    "One-Hot Encoding: Encode categorical variables, if any, using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "fJc-z4mVpITS"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Onehot encoding the target variable\n",
    "encoder = OneHotEncoder()\n",
    "y_encoded = encoder.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Convert encoded target back to DataFrame (optional)\n",
    "y_encoded_df = pd.DataFrame(y_encoded.toarray(), columns=encoder.categories_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuCG8uzwqyWR"
   },
   "source": [
    "Splitting Data: Split the dataset into training and testing sets to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "o-GVGHJZp32Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X_engineered, y_encoded_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70xe8SJWrUL5"
   },
   "source": [
    "#Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "W_BMDxJTp3zB"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "RU082n2up3wJ"
   },
   "outputs": [],
   "source": [
    "# Constructing the ANN model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Hidden layer with 64 neurons and ReLU activation\n",
    "    Dense(26, activation='softmax')  # Output layer with 26 neurons (one for each class) and softmax activation\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrKI2gVUsP75"
   },
   "source": [
    "The model is compiled using the Adam optimizer and categorical cross-entropy loss function, which is commonly used for multi-class classification tasks.\n",
    "\n",
    "The model is trained on the training data for 10 epochs with a batch size of 32 and a validation split of 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "mR7xPRUCp3tB"
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-405r1Mbp3qH",
    "outputId": "9a274743-9df0-43c2-8ab6-c265ef45318a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - accuracy: 0.0430 - loss: nan - val_accuracy: 0.0449 - val_loss: nan\n",
      "Epoch 2/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391us/step - accuracy: 0.0402 - loss: nan - val_accuracy: 0.0449 - val_loss: nan\n",
      "Epoch 3/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388us/step - accuracy: 0.0419 - loss: nan - val_accuracy: 0.0449 - val_loss: nan\n",
      "Epoch 4/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390us/step - accuracy: 0.0363 - loss: nan - val_accuracy: 0.0449 - val_loss: nan\n",
      "Epoch 5/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389us/step - accuracy: 0.0425 - loss: nan - val_accuracy: 0.0449 - val_loss: nan\n",
      "Epoch 6/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391us/step - accuracy: 0.0384 - loss: nan - val_accuracy: 0.0449 - val_loss: nan\n",
      "Epoch 7/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391us/step - accuracy: 0.0392 - loss: nan - val_accuracy: 0.0449 - val_loss: nan\n",
      "Epoch 8/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390us/step - accuracy: 0.0385 - loss: nan - val_accuracy: 0.0449 - val_loss: nan\n",
      "Epoch 9/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393us/step - accuracy: 0.0368 - loss: nan - val_accuracy: 0.0449 - val_loss: nan\n",
      "Epoch 10/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389us/step - accuracy: 0.0411 - loss: nan - val_accuracy: 0.0449 - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "history = model.fit(X_train, y_train_encoded, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1SJ-fUSHrxiJ",
    "outputId": "660da492-df1a-4469-f0e2-1ebe8777a4c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439us/step - accuracy: 0.0422 - loss: nan\n",
      "Test Loss: nan\n",
      "Test Accuracy: 0.038564540445804596\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULFHIQOyt0DZ"
   },
   "source": [
    "#It seems that the model encountered an issue during training, as indicated by the presence of \"nan\" (not a number) values in both the training and validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGzvdp3qt30Z"
   },
   "source": [
    "Diagnose and address this issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y9XsPHjht6CH",
    "outputId": "a7936ea2-1c69-4ac4-bfc7-bbd9b8acc5d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in features:\n",
      " xbox         0\n",
      "ybox         0\n",
      "width        0\n",
      "height       0\n",
      "onpix        0\n",
      "xbar         0\n",
      "ybar         0\n",
      "x2bar        0\n",
      "y2bar        0\n",
      "xybar        0\n",
      "x2ybar       0\n",
      "xy2bar       0\n",
      "xedge        0\n",
      "xedgey       0\n",
      "yedge        0\n",
      "yedgex       0\n",
      "area      1182\n",
      "dtype: int64\n",
      "\n",
      "Missing values in target variable:\n",
      " A    0\n",
      "B    0\n",
      "C    0\n",
      "D    0\n",
      "E    0\n",
      "F    0\n",
      "G    0\n",
      "H    0\n",
      "I    0\n",
      "J    0\n",
      "K    0\n",
      "L    0\n",
      "M    0\n",
      "N    0\n",
      "O    0\n",
      "P    0\n",
      "Q    0\n",
      "R    0\n",
      "S    0\n",
      "T    0\n",
      "U    0\n",
      "V    0\n",
      "W    0\n",
      "X    0\n",
      "Y    0\n",
      "Z    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the features\n",
    "missing_values_features = X_engineered.isnull().sum()\n",
    "print(\"Missing values in features:\\n\", missing_values_features)\n",
    "\n",
    "# Check for missing values in the target variable\n",
    "missing_values_target = y_encoded_df.isnull().sum()\n",
    "print(\"\\nMissing values in target variable:\\n\", missing_values_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "w6bCPf68s7Go"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an imputer object with strategy='mean'\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Impute missing values in the 'area' feature\n",
    "X_engineered_imputed = X_engineered.copy()  # Create a copy of the DataFrame to avoid modifying the original data\n",
    "X_engineered_imputed['area'] = imputer.fit_transform(X_engineered[['area']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRNkf4e9uQXz"
   },
   "source": [
    "proceeding with training your neural network model using the preprocessed data with imputed missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UOLU0p9qs7Dp",
    "outputId": "68490224-e552-4f32-b05b-4da90c579b19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - accuracy: 0.1405 - loss: 4.2014 - val_accuracy: 0.5022 - val_loss: 1.8768\n",
      "Epoch 2/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385us/step - accuracy: 0.5587 - loss: 1.6708 - val_accuracy: 0.6016 - val_loss: 1.3710\n",
      "Epoch 3/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385us/step - accuracy: 0.6634 - loss: 1.2489 - val_accuracy: 0.6850 - val_loss: 1.1354\n",
      "Epoch 4/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - accuracy: 0.7021 - loss: 1.0725 - val_accuracy: 0.6943 - val_loss: 1.0525\n",
      "Epoch 5/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - accuracy: 0.7278 - loss: 0.9642 - val_accuracy: 0.7258 - val_loss: 0.9691\n",
      "Epoch 6/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437us/step - accuracy: 0.7511 - loss: 0.8822 - val_accuracy: 0.7459 - val_loss: 0.9210\n",
      "Epoch 7/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401us/step - accuracy: 0.7683 - loss: 0.8220 - val_accuracy: 0.7523 - val_loss: 0.8782\n",
      "Epoch 8/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391us/step - accuracy: 0.7773 - loss: 0.7800 - val_accuracy: 0.7657 - val_loss: 0.8371\n",
      "Epoch 9/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389us/step - accuracy: 0.7925 - loss: 0.7246 - val_accuracy: 0.7680 - val_loss: 0.8066\n",
      "Epoch 10/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390us/step - accuracy: 0.7986 - loss: 0.7016 - val_accuracy: 0.7790 - val_loss: 0.7744\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295us/step - accuracy: 0.7874 - loss: 0.7533\n",
      "Test Loss: 0.7417078018188477\n",
      "Test Accuracy: 0.7868238091468811\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train_imputed, X_test_imputed, y_train_encoded, y_test_encoded = train_test_split(X_engineered_imputed, y_encoded_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Constructing the ANN model\n",
    "model_imputed = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_imputed.shape[1],)),  # Hidden layer with 64 neurons and ReLU activation\n",
    "    Dense(26, activation='softmax')  # Output layer with 26 neurons (one for each class) and softmax activation\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model_imputed.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history_imputed = model_imputed.fit(X_train_imputed, y_train_encoded, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss_imputed, test_accuracy_imputed = model_imputed.evaluate(X_test_imputed, y_test_encoded)\n",
    "print(\"Test Loss:\", test_loss_imputed)\n",
    "print(\"Test Accuracy:\", test_accuracy_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwiXtS17ugCb"
   },
   "source": [
    "These results indicate that the model performed reasonably well on both the training and test datasets, with an accuracy of approximately 77.77% on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f4H4CUquxcq"
   },
   "source": [
    "#Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GqmC-VkXzoVy",
    "outputId": "6d47f1ad-bfd0-4117-db2e-6792fcbb0d55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with layers=1, neurons=32, activation=relu, dropout_rate=0.1, learning_rate=0.001\n",
      "Training model with layers=1, neurons=32, activation=relu, dropout_rate=0.1, learning_rate=0.01\n",
      "Training model with layers=1, neurons=32, activation=relu, dropout_rate=0.1, learning_rate=0.1\n",
      "Training model with layers=1, neurons=32, activation=relu, dropout_rate=0.2, learning_rate=0.001\n",
      "Training model with layers=1, neurons=32, activation=relu, dropout_rate=0.2, learning_rate=0.01\n",
      "Training model with layers=1, neurons=32, activation=relu, dropout_rate=0.2, learning_rate=0.1\n",
      "Training model with layers=1, neurons=32, activation=relu, dropout_rate=0.3, learning_rate=0.001\n",
      "Training model with layers=1, neurons=32, activation=relu, dropout_rate=0.3, learning_rate=0.01\n",
      "Training model with layers=1, neurons=32, activation=relu, dropout_rate=0.3, learning_rate=0.1\n",
      "Training model with layers=1, neurons=32, activation=tanh, dropout_rate=0.1, learning_rate=0.001\n",
      "Training model with layers=1, neurons=32, activation=tanh, dropout_rate=0.1, learning_rate=0.01\n",
      "Training model with layers=1, neurons=32, activation=tanh, dropout_rate=0.1, learning_rate=0.1\n",
      "Training model with layers=1, neurons=32, activation=tanh, dropout_rate=0.2, learning_rate=0.001\n",
      "Training model with layers=1, neurons=32, activation=tanh, dropout_rate=0.2, learning_rate=0.01\n",
      "Training model with layers=1, neurons=32, activation=tanh, dropout_rate=0.2, learning_rate=0.1\n",
      "Training model with layers=1, neurons=32, activation=tanh, dropout_rate=0.3, learning_rate=0.001\n",
      "Training model with layers=1, neurons=32, activation=tanh, dropout_rate=0.3, learning_rate=0.01\n",
      "Training model with layers=1, neurons=32, activation=tanh, dropout_rate=0.3, learning_rate=0.1\n",
      "Training model with layers=1, neurons=64, activation=relu, dropout_rate=0.1, learning_rate=0.001\n",
      "Training model with layers=1, neurons=64, activation=relu, dropout_rate=0.1, learning_rate=0.01\n",
      "Training model with layers=1, neurons=64, activation=relu, dropout_rate=0.1, learning_rate=0.1\n",
      "Training model with layers=1, neurons=64, activation=relu, dropout_rate=0.2, learning_rate=0.001\n",
      "Training model with layers=1, neurons=64, activation=relu, dropout_rate=0.2, learning_rate=0.01\n",
      "Training model with layers=1, neurons=64, activation=relu, dropout_rate=0.2, learning_rate=0.1\n",
      "Training model with layers=1, neurons=64, activation=relu, dropout_rate=0.3, learning_rate=0.001\n",
      "Training model with layers=1, neurons=64, activation=relu, dropout_rate=0.3, learning_rate=0.01\n",
      "Training model with layers=1, neurons=64, activation=relu, dropout_rate=0.3, learning_rate=0.1\n",
      "Training model with layers=1, neurons=64, activation=tanh, dropout_rate=0.1, learning_rate=0.001\n",
      "Training model with layers=1, neurons=64, activation=tanh, dropout_rate=0.1, learning_rate=0.01\n",
      "Training model with layers=1, neurons=64, activation=tanh, dropout_rate=0.1, learning_rate=0.1\n",
      "Training model with layers=1, neurons=64, activation=tanh, dropout_rate=0.2, learning_rate=0.001\n",
      "Training model with layers=1, neurons=64, activation=tanh, dropout_rate=0.2, learning_rate=0.01\n",
      "Training model with layers=1, neurons=64, activation=tanh, dropout_rate=0.2, learning_rate=0.1\n",
      "Training model with layers=1, neurons=64, activation=tanh, dropout_rate=0.3, learning_rate=0.001\n",
      "Training model with layers=1, neurons=64, activation=tanh, dropout_rate=0.3, learning_rate=0.01\n",
      "Training model with layers=1, neurons=64, activation=tanh, dropout_rate=0.3, learning_rate=0.1\n",
      "Training model with layers=1, neurons=128, activation=relu, dropout_rate=0.1, learning_rate=0.001\n",
      "Training model with layers=1, neurons=128, activation=relu, dropout_rate=0.1, learning_rate=0.01\n",
      "Training model with layers=1, neurons=128, activation=relu, dropout_rate=0.1, learning_rate=0.1\n",
      "Training model with layers=1, neurons=128, activation=relu, dropout_rate=0.2, learning_rate=0.001\n",
      "Training model with layers=1, neurons=128, activation=relu, dropout_rate=0.2, learning_rate=0.01\n",
      "Training model with layers=1, neurons=128, activation=relu, dropout_rate=0.2, learning_rate=0.1\n",
      "Training model with layers=1, neurons=128, activation=relu, dropout_rate=0.3, learning_rate=0.001\n",
      "Training model with layers=1, neurons=128, activation=relu, dropout_rate=0.3, learning_rate=0.01\n",
      "Training model with layers=1, neurons=128, activation=relu, dropout_rate=0.3, learning_rate=0.1\n",
      "Training model with layers=1, neurons=128, activation=tanh, dropout_rate=0.1, learning_rate=0.001\n",
      "Training model with layers=1, neurons=128, activation=tanh, dropout_rate=0.1, learning_rate=0.01\n",
      "Training model with layers=1, neurons=128, activation=tanh, dropout_rate=0.1, learning_rate=0.1\n",
      "Training model with layers=1, neurons=128, activation=tanh, dropout_rate=0.2, learning_rate=0.001\n",
      "Training model with layers=1, neurons=128, activation=tanh, dropout_rate=0.2, learning_rate=0.01\n",
      "Training model with layers=1, neurons=128, activation=tanh, dropout_rate=0.2, learning_rate=0.1\n",
      "Training model with layers=1, neurons=128, activation=tanh, dropout_rate=0.3, learning_rate=0.001\n",
      "Training model with layers=1, neurons=128, activation=tanh, dropout_rate=0.3, learning_rate=0.01\n",
      "Training model with layers=1, neurons=128, activation=tanh, dropout_rate=0.3, learning_rate=0.1\n",
      "Training model with layers=2, neurons=32, activation=relu, dropout_rate=0.1, learning_rate=0.001\n",
      "Training model with layers=2, neurons=32, activation=relu, dropout_rate=0.1, learning_rate=0.01\n",
      "Training model with layers=2, neurons=32, activation=relu, dropout_rate=0.1, learning_rate=0.1\n",
      "Training model with layers=2, neurons=32, activation=relu, dropout_rate=0.2, learning_rate=0.001\n",
      "Training model with layers=2, neurons=32, activation=relu, dropout_rate=0.2, learning_rate=0.01\n",
      "Training model with layers=2, neurons=32, activation=relu, dropout_rate=0.2, learning_rate=0.1\n",
      "Training model with layers=2, neurons=32, activation=relu, dropout_rate=0.3, learning_rate=0.001\n",
      "Training model with layers=2, neurons=32, activation=relu, dropout_rate=0.3, learning_rate=0.01\n",
      "Training model with layers=2, neurons=32, activation=relu, dropout_rate=0.3, learning_rate=0.1\n",
      "Training model with layers=2, neurons=32, activation=tanh, dropout_rate=0.1, learning_rate=0.001\n",
      "Training model with layers=2, neurons=32, activation=tanh, dropout_rate=0.1, learning_rate=0.01\n",
      "Training model with layers=2, neurons=32, activation=tanh, dropout_rate=0.1, learning_rate=0.1\n",
      "Training model with layers=2, neurons=32, activation=tanh, dropout_rate=0.2, learning_rate=0.001\n",
      "Training model with layers=2, neurons=32, activation=tanh, dropout_rate=0.2, learning_rate=0.01\n",
      "Training model with layers=2, neurons=32, activation=tanh, dropout_rate=0.2, learning_rate=0.1\n",
      "Training model with layers=2, neurons=32, activation=tanh, dropout_rate=0.3, learning_rate=0.001\n",
      "Training model with layers=2, neurons=32, activation=tanh, dropout_rate=0.3, learning_rate=0.01\n",
      "Training model with layers=2, neurons=32, activation=tanh, dropout_rate=0.3, learning_rate=0.1\n",
      "Training model with layers=2, neurons=64, activation=relu, dropout_rate=0.1, learning_rate=0.001\n",
      "Training model with layers=2, neurons=64, activation=relu, dropout_rate=0.1, learning_rate=0.01\n",
      "Training model with layers=2, neurons=64, activation=relu, dropout_rate=0.1, learning_rate=0.1\n",
      "Training model with layers=2, neurons=64, activation=relu, dropout_rate=0.2, learning_rate=0.001\n",
      "Training model with layers=2, neurons=64, activation=relu, dropout_rate=0.2, learning_rate=0.01\n",
      "Training model with layers=2, neurons=64, activation=relu, dropout_rate=0.2, learning_rate=0.1\n",
      "Training model with layers=2, neurons=64, activation=relu, dropout_rate=0.3, learning_rate=0.001\n",
      "Training model with layers=2, neurons=64, activation=relu, dropout_rate=0.3, learning_rate=0.01\n",
      "Training model with layers=2, neurons=64, activation=relu, dropout_rate=0.3, learning_rate=0.1\n",
      "Training model with layers=2, neurons=64, activation=tanh, dropout_rate=0.1, learning_rate=0.001\n",
      "Training model with layers=2, neurons=64, activation=tanh, dropout_rate=0.1, learning_rate=0.01\n",
      "Training model with layers=2, neurons=64, activation=tanh, dropout_rate=0.1, learning_rate=0.1\n",
      "Training model with layers=2, neurons=64, activation=tanh, dropout_rate=0.2, learning_rate=0.001\n",
      "Training model with layers=2, neurons=64, activation=tanh, dropout_rate=0.2, learning_rate=0.01\n",
      "Training model with layers=2, neurons=64, activation=tanh, dropout_rate=0.2, learning_rate=0.1\n",
      "Training model with layers=2, neurons=64, activation=tanh, dropout_rate=0.3, learning_rate=0.001\n",
      "Training model with layers=2, neurons=64, activation=tanh, dropout_rate=0.3, learning_rate=0.01\n",
      "Training model with layers=2, neurons=64, activation=tanh, dropout_rate=0.3, learning_rate=0.1\n",
      "Training model with layers=2, neurons=128, activation=relu, dropout_rate=0.1, learning_rate=0.001\n",
      "Training model with layers=2, neurons=128, activation=relu, dropout_rate=0.1, learning_rate=0.01\n",
      "Training model with layers=2, neurons=128, activation=relu, dropout_rate=0.1, learning_rate=0.1\n",
      "Training model with layers=2, neurons=128, activation=relu, dropout_rate=0.2, learning_rate=0.001\n",
      "Training model with layers=2, neurons=128, activation=relu, dropout_rate=0.2, learning_rate=0.01\n",
      "Training model with layers=2, neurons=128, activation=relu, dropout_rate=0.2, learning_rate=0.1\n",
      "Training model with layers=2, neurons=128, activation=relu, dropout_rate=0.3, learning_rate=0.001\n",
      "Training model with layers=2, neurons=128, activation=relu, dropout_rate=0.3, learning_rate=0.01\n",
      "Training model with layers=2, neurons=128, activation=relu, dropout_rate=0.3, learning_rate=0.1\n",
      "Training model with layers=2, neurons=128, activation=tanh, dropout_rate=0.1, learning_rate=0.001\n",
      "Training model with layers=2, neurons=128, activation=tanh, dropout_rate=0.1, learning_rate=0.01\n",
      "Training model with layers=2, neurons=128, activation=tanh, dropout_rate=0.1, learning_rate=0.1\n",
      "Training model with layers=2, neurons=128, activation=tanh, dropout_rate=0.2, learning_rate=0.001\n",
      "Training model with layers=2, neurons=128, activation=tanh, dropout_rate=0.2, learning_rate=0.01\n",
      "Training model with layers=2, neurons=128, activation=tanh, dropout_rate=0.2, learning_rate=0.1\n",
      "Training model with layers=2, neurons=128, activation=tanh, dropout_rate=0.3, learning_rate=0.001\n",
      "Training model with layers=2, neurons=128, activation=tanh, dropout_rate=0.3, learning_rate=0.01\n",
      "Training model with layers=2, neurons=128, activation=tanh, dropout_rate=0.3, learning_rate=0.1\n",
      "Training model with layers=3, neurons=32, activation=relu, dropout_rate=0.1, learning_rate=0.001\n",
      "Training model with layers=3, neurons=32, activation=relu, dropout_rate=0.1, learning_rate=0.01\n",
      "Training model with layers=3, neurons=32, activation=relu, dropout_rate=0.1, learning_rate=0.1\n",
      "Training model with layers=3, neurons=32, activation=relu, dropout_rate=0.2, learning_rate=0.001\n",
      "Training model with layers=3, neurons=32, activation=relu, dropout_rate=0.2, learning_rate=0.01\n",
      "Training model with layers=3, neurons=32, activation=relu, dropout_rate=0.2, learning_rate=0.1\n",
      "Training model with layers=3, neurons=32, activation=relu, dropout_rate=0.3, learning_rate=0.001\n",
      "Training model with layers=3, neurons=32, activation=relu, dropout_rate=0.3, learning_rate=0.01\n",
      "Training model with layers=3, neurons=32, activation=relu, dropout_rate=0.3, learning_rate=0.1\n",
      "Training model with layers=3, neurons=32, activation=tanh, dropout_rate=0.1, learning_rate=0.001\n",
      "Training model with layers=3, neurons=32, activation=tanh, dropout_rate=0.1, learning_rate=0.01\n",
      "Training model with layers=3, neurons=32, activation=tanh, dropout_rate=0.1, learning_rate=0.1\n",
      "Training model with layers=3, neurons=32, activation=tanh, dropout_rate=0.2, learning_rate=0.001\n",
      "Training model with layers=3, neurons=32, activation=tanh, dropout_rate=0.2, learning_rate=0.01\n",
      "Training model with layers=3, neurons=32, activation=tanh, dropout_rate=0.2, learning_rate=0.1\n",
      "Training model with layers=3, neurons=32, activation=tanh, dropout_rate=0.3, learning_rate=0.001\n",
      "Training model with layers=3, neurons=32, activation=tanh, dropout_rate=0.3, learning_rate=0.01\n",
      "Training model with layers=3, neurons=32, activation=tanh, dropout_rate=0.3, learning_rate=0.1\n",
      "Training model with layers=3, neurons=64, activation=relu, dropout_rate=0.1, learning_rate=0.001\n",
      "Training model with layers=3, neurons=64, activation=relu, dropout_rate=0.1, learning_rate=0.01\n",
      "Training model with layers=3, neurons=64, activation=relu, dropout_rate=0.1, learning_rate=0.1\n",
      "Training model with layers=3, neurons=64, activation=relu, dropout_rate=0.2, learning_rate=0.001\n",
      "Training model with layers=3, neurons=64, activation=relu, dropout_rate=0.2, learning_rate=0.01\n",
      "Training model with layers=3, neurons=64, activation=relu, dropout_rate=0.2, learning_rate=0.1\n",
      "Training model with layers=3, neurons=64, activation=relu, dropout_rate=0.3, learning_rate=0.001\n",
      "Training model with layers=3, neurons=64, activation=relu, dropout_rate=0.3, learning_rate=0.01\n",
      "Training model with layers=3, neurons=64, activation=relu, dropout_rate=0.3, learning_rate=0.1\n",
      "Training model with layers=3, neurons=64, activation=tanh, dropout_rate=0.1, learning_rate=0.001\n",
      "Training model with layers=3, neurons=64, activation=tanh, dropout_rate=0.1, learning_rate=0.01\n",
      "Training model with layers=3, neurons=64, activation=tanh, dropout_rate=0.1, learning_rate=0.1\n",
      "Training model with layers=3, neurons=64, activation=tanh, dropout_rate=0.2, learning_rate=0.001\n",
      "Training model with layers=3, neurons=64, activation=tanh, dropout_rate=0.2, learning_rate=0.01\n",
      "Training model with layers=3, neurons=64, activation=tanh, dropout_rate=0.2, learning_rate=0.1\n",
      "Training model with layers=3, neurons=64, activation=tanh, dropout_rate=0.3, learning_rate=0.001\n",
      "Training model with layers=3, neurons=64, activation=tanh, dropout_rate=0.3, learning_rate=0.01\n",
      "Training model with layers=3, neurons=64, activation=tanh, dropout_rate=0.3, learning_rate=0.1\n",
      "Training model with layers=3, neurons=128, activation=relu, dropout_rate=0.1, learning_rate=0.001\n",
      "Training model with layers=3, neurons=128, activation=relu, dropout_rate=0.1, learning_rate=0.01\n",
      "Training model with layers=3, neurons=128, activation=relu, dropout_rate=0.1, learning_rate=0.1\n",
      "Training model with layers=3, neurons=128, activation=relu, dropout_rate=0.2, learning_rate=0.001\n",
      "Training model with layers=3, neurons=128, activation=relu, dropout_rate=0.2, learning_rate=0.01\n",
      "Training model with layers=3, neurons=128, activation=relu, dropout_rate=0.2, learning_rate=0.1\n",
      "Training model with layers=3, neurons=128, activation=relu, dropout_rate=0.3, learning_rate=0.001\n",
      "Training model with layers=3, neurons=128, activation=relu, dropout_rate=0.3, learning_rate=0.01\n",
      "Training model with layers=3, neurons=128, activation=relu, dropout_rate=0.3, learning_rate=0.1\n",
      "Training model with layers=3, neurons=128, activation=tanh, dropout_rate=0.1, learning_rate=0.001\n",
      "Training model with layers=3, neurons=128, activation=tanh, dropout_rate=0.1, learning_rate=0.01\n",
      "Training model with layers=3, neurons=128, activation=tanh, dropout_rate=0.1, learning_rate=0.1\n",
      "Training model with layers=3, neurons=128, activation=tanh, dropout_rate=0.2, learning_rate=0.001\n",
      "Training model with layers=3, neurons=128, activation=tanh, dropout_rate=0.2, learning_rate=0.01\n",
      "Training model with layers=3, neurons=128, activation=tanh, dropout_rate=0.2, learning_rate=0.1\n",
      "Training model with layers=3, neurons=128, activation=tanh, dropout_rate=0.3, learning_rate=0.001\n",
      "Training model with layers=3, neurons=128, activation=tanh, dropout_rate=0.3, learning_rate=0.01\n",
      "Training model with layers=3, neurons=128, activation=tanh, dropout_rate=0.3, learning_rate=0.1\n",
      "Layers: 1, Neurons: 32, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.001, Test Loss: 0.887102484703064, Test Accuracy: 0.7391537427902222\n",
      "Layers: 1, Neurons: 32, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.01, Test Loss: 0.6222293972969055, Test Accuracy: 0.8074451088905334\n",
      "Layers: 1, Neurons: 32, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.1, Test Loss: 2.3255250453948975, Test Accuracy: 0.27637922763824463\n",
      "Layers: 1, Neurons: 32, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.001, Test Loss: 0.9588558673858643, Test Accuracy: 0.7287091612815857\n",
      "Layers: 1, Neurons: 32, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.01, Test Loss: 0.716574490070343, Test Accuracy: 0.7739689350128174\n",
      "Layers: 1, Neurons: 32, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.1, Test Loss: 2.9725561141967773, Test Accuracy: 0.12346009910106659\n",
      "Layers: 1, Neurons: 32, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.001, Test Loss: 1.0024296045303345, Test Accuracy: 0.7271022796630859\n",
      "Layers: 1, Neurons: 32, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.01, Test Loss: 0.7516649961471558, Test Accuracy: 0.7769148349761963\n",
      "Layers: 1, Neurons: 32, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.1, Test Loss: 3.1117002964019775, Test Accuracy: 0.09159078449010849\n",
      "Layers: 1, Neurons: 32, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.001, Test Loss: 0.9192357659339905, Test Accuracy: 0.7461167573928833\n",
      "Layers: 1, Neurons: 32, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.01, Test Loss: 0.6112685799598694, Test Accuracy: 0.8149437308311462\n",
      "Layers: 1, Neurons: 32, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.1, Test Loss: 2.01499605178833, Test Accuracy: 0.4799143075942993\n",
      "Layers: 1, Neurons: 32, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.001, Test Loss: 0.9341129064559937, Test Accuracy: 0.7525441646575928\n",
      "Layers: 1, Neurons: 32, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.01, Test Loss: 0.6843875050544739, Test Accuracy: 0.7994108200073242\n",
      "Layers: 1, Neurons: 32, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.1, Test Loss: 1.8073921203613281, Test Accuracy: 0.4978575110435486\n",
      "Layers: 1, Neurons: 32, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.001, Test Loss: 0.9919750690460205, Test Accuracy: 0.729780375957489\n",
      "Layers: 1, Neurons: 32, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.01, Test Loss: 0.7576316595077515, Test Accuracy: 0.7811998128890991\n",
      "Layers: 1, Neurons: 32, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.1, Test Loss: 1.675308108329773, Test Accuracy: 0.5524906516075134\n",
      "Layers: 1, Neurons: 64, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.001, Test Loss: 0.7389196753501892, Test Accuracy: 0.7846813201904297\n",
      "Layers: 1, Neurons: 64, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.01, Test Loss: 0.5562660694122314, Test Accuracy: 0.8363685011863708\n",
      "Layers: 1, Neurons: 64, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.1, Test Loss: 2.4764955043792725, Test Accuracy: 0.22897696495056152\n",
      "Layers: 1, Neurons: 64, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.001, Test Loss: 0.7299628257751465, Test Accuracy: 0.7943224310874939\n",
      "Layers: 1, Neurons: 64, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.01, Test Loss: 0.5458318591117859, Test Accuracy: 0.8299410939216614\n",
      "Layers: 1, Neurons: 64, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.1, Test Loss: 2.96510910987854, Test Accuracy: 0.14220674335956573\n",
      "Layers: 1, Neurons: 64, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.001, Test Loss: 0.7744612097740173, Test Accuracy: 0.7803963422775269\n",
      "Layers: 1, Neurons: 64, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.01, Test Loss: 0.594996988773346, Test Accuracy: 0.8197643160820007\n",
      "Layers: 1, Neurons: 64, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.1, Test Loss: 3.2064015865325928, Test Accuracy: 0.06186395138502121\n",
      "Layers: 1, Neurons: 64, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.001, Test Loss: 0.77605140209198, Test Accuracy: 0.7809319496154785\n",
      "Layers: 1, Neurons: 64, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.01, Test Loss: 0.5046432018280029, Test Accuracy: 0.8462774753570557\n",
      "Layers: 1, Neurons: 64, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.1, Test Loss: 2.1610021591186523, Test Accuracy: 0.5693626403808594\n",
      "Layers: 1, Neurons: 64, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.001, Test Loss: 0.7913902997970581, Test Accuracy: 0.7793251276016235\n",
      "Layers: 1, Neurons: 64, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.01, Test Loss: 0.6016313433647156, Test Accuracy: 0.8216390013694763\n",
      "Layers: 1, Neurons: 64, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.1, Test Loss: 2.5467591285705566, Test Accuracy: 0.5182110071182251\n",
      "Layers: 1, Neurons: 64, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.001, Test Loss: 0.8411168456077576, Test Accuracy: 0.7720942497253418\n",
      "Layers: 1, Neurons: 64, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.01, Test Loss: 0.6379631161689758, Test Accuracy: 0.8069095015525818\n",
      "Layers: 1, Neurons: 64, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.1, Test Loss: 2.706681489944458, Test Accuracy: 0.4874129593372345\n",
      "Layers: 1, Neurons: 128, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.001, Test Loss: 0.5852960348129272, Test Accuracy: 0.830476701259613\n",
      "Layers: 1, Neurons: 128, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.01, Test Loss: 0.5517677664756775, Test Accuracy: 0.836100697517395\n",
      "Layers: 1, Neurons: 128, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.1, Test Loss: 2.4991936683654785, Test Accuracy: 0.2287091612815857\n",
      "Layers: 1, Neurons: 128, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.001, Test Loss: 0.5951442718505859, Test Accuracy: 0.8344938158988953\n",
      "Layers: 1, Neurons: 128, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.01, Test Loss: 0.4930240213871002, Test Accuracy: 0.8567219972610474\n",
      "Layers: 1, Neurons: 128, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.1, Test Loss: 3.0561459064483643, Test Accuracy: 0.12024638801813126\n",
      "Layers: 1, Neurons: 128, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.001, Test Loss: 0.6205770373344421, Test Accuracy: 0.8192287087440491\n",
      "Layers: 1, Neurons: 128, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.01, Test Loss: 0.5618518590927124, Test Accuracy: 0.8296732902526855\n",
      "Layers: 1, Neurons: 128, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.1, Test Loss: 3.2500948905944824, Test Accuracy: 0.0752544179558754\n",
      "Layers: 1, Neurons: 128, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.001, Test Loss: 0.6709780097007751, Test Accuracy: 0.8117300271987915\n",
      "Layers: 1, Neurons: 128, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.01, Test Loss: 0.5278730392456055, Test Accuracy: 0.8454740047454834\n",
      "Layers: 1, Neurons: 128, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.1, Test Loss: 4.547337055206299, Test Accuracy: 0.46277451515197754\n",
      "Layers: 1, Neurons: 128, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.001, Test Loss: 0.6989551186561584, Test Accuracy: 0.8031601309776306\n",
      "Layers: 1, Neurons: 128, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.01, Test Loss: 0.5747841000556946, Test Accuracy: 0.8369041085243225\n",
      "Layers: 1, Neurons: 128, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.1, Test Loss: 4.819603443145752, Test Accuracy: 0.45420461893081665\n",
      "Layers: 1, Neurons: 128, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.001, Test Loss: 0.7721990942955017, Test Accuracy: 0.7811998128890991\n",
      "Layers: 1, Neurons: 128, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.01, Test Loss: 0.629918098449707, Test Accuracy: 0.8090519309043884\n",
      "Layers: 1, Neurons: 128, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.1, Test Loss: 5.730398654937744, Test Accuracy: 0.4276914894580841\n",
      "Layers: 2, Neurons: 32, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.001, Test Loss: 0.8313479423522949, Test Accuracy: 0.7520085573196411\n",
      "Layers: 2, Neurons: 32, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.01, Test Loss: 0.7756895422935486, Test Accuracy: 0.758971631526947\n",
      "Layers: 2, Neurons: 32, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.1, Test Loss: 2.8374524116516113, Test Accuracy: 0.09989287704229355\n",
      "Layers: 2, Neurons: 32, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.001, Test Loss: 0.9915331602096558, Test Accuracy: 0.7040706872940063\n",
      "Layers: 2, Neurons: 32, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.01, Test Loss: 0.8690555095672607, Test Accuracy: 0.7319228649139404\n",
      "Layers: 2, Neurons: 32, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.1, Test Loss: 3.2774033546447754, Test Accuracy: 0.04311729967594147\n",
      "Layers: 2, Neurons: 32, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.001, Test Loss: 1.0758678913116455, Test Accuracy: 0.6957685947418213\n",
      "Layers: 2, Neurons: 32, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.01, Test Loss: 0.9750365018844604, Test Accuracy: 0.7038028836250305\n",
      "Layers: 2, Neurons: 32, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.1, Test Loss: 3.2730915546417236, Test Accuracy: 0.03910015895962715\n",
      "Layers: 2, Neurons: 32, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.001, Test Loss: 0.8044165372848511, Test Accuracy: 0.7704874277114868\n",
      "Layers: 2, Neurons: 32, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.01, Test Loss: 0.6145945191383362, Test Accuracy: 0.8173540234565735\n",
      "Layers: 2, Neurons: 32, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.1, Test Loss: 2.935593605041504, Test Accuracy: 0.2346009612083435\n",
      "Layers: 2, Neurons: 32, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.001, Test Loss: 0.9091039896011353, Test Accuracy: 0.7356721758842468\n",
      "Layers: 2, Neurons: 32, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.01, Test Loss: 0.7301832437515259, Test Accuracy: 0.7782539129257202\n",
      "Layers: 2, Neurons: 32, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.1, Test Loss: 2.9736688137054443, Test Accuracy: 0.1738082468509674\n",
      "Layers: 2, Neurons: 32, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.001, Test Loss: 1.0490038394927979, Test Accuracy: 0.7000535726547241\n",
      "Layers: 2, Neurons: 32, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.01, Test Loss: 0.8983781933784485, Test Accuracy: 0.7477236390113831\n",
      "Layers: 2, Neurons: 32, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.1, Test Loss: 3.3604965209960938, Test Accuracy: 0.12881627678871155\n",
      "Layers: 2, Neurons: 64, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.001, Test Loss: 0.5185210704803467, Test Accuracy: 0.8462774753570557\n",
      "Layers: 2, Neurons: 64, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.01, Test Loss: 0.5742256045341492, Test Accuracy: 0.8202999234199524\n",
      "Layers: 2, Neurons: 64, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.1, Test Loss: 3.2779948711395264, Test Accuracy: 0.035083021968603134\n",
      "Layers: 2, Neurons: 64, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.001, Test Loss: 0.6530235409736633, Test Accuracy: 0.8058382272720337\n",
      "Layers: 2, Neurons: 64, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.01, Test Loss: 0.7042413949966431, Test Accuracy: 0.7779860496520996\n",
      "Layers: 2, Neurons: 64, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.1, Test Loss: 3.285923480987549, Test Accuracy: 0.03883235156536102\n",
      "Layers: 2, Neurons: 64, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.001, Test Loss: 0.7766963839530945, Test Accuracy: 0.7629887461662292\n",
      "Layers: 2, Neurons: 64, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.01, Test Loss: 0.8361418843269348, Test Accuracy: 0.7439742684364319\n",
      "Layers: 2, Neurons: 64, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.1, Test Loss: 3.2793021202087402, Test Accuracy: 0.03910015895962715\n",
      "Layers: 2, Neurons: 64, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.001, Test Loss: 0.5959495902061462, Test Accuracy: 0.8186931014060974\n",
      "Layers: 2, Neurons: 64, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.01, Test Loss: 0.5554100871086121, Test Accuracy: 0.8229780197143555\n",
      "Layers: 2, Neurons: 64, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.1, Test Loss: 3.5686092376708984, Test Accuracy: 0.138993039727211\n",
      "Layers: 2, Neurons: 64, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.001, Test Loss: 0.6557341814041138, Test Accuracy: 0.810926616191864\n",
      "Layers: 2, Neurons: 64, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.01, Test Loss: 0.6055964231491089, Test Accuracy: 0.8173540234565735\n",
      "Layers: 2, Neurons: 64, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.1, Test Loss: 3.5333733558654785, Test Accuracy: 0.10846277326345444\n",
      "Layers: 2, Neurons: 64, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.001, Test Loss: 0.7684995532035828, Test Accuracy: 0.7763792276382446\n",
      "Layers: 2, Neurons: 64, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.01, Test Loss: 0.7130207419395447, Test Accuracy: 0.7836100459098816\n",
      "Layers: 2, Neurons: 64, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.1, Test Loss: 3.930877685546875, Test Accuracy: 0.1052490621805191\n",
      "Layers: 2, Neurons: 128, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.001, Test Loss: 0.3557021915912628, Test Accuracy: 0.888055682182312\n",
      "Layers: 2, Neurons: 128, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.01, Test Loss: 0.4998827576637268, Test Accuracy: 0.8446705937385559\n",
      "Layers: 2, Neurons: 128, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.1, Test Loss: 3.276582956314087, Test Accuracy: 0.039903588593006134\n",
      "Layers: 2, Neurons: 128, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.001, Test Loss: 0.42530497908592224, Test Accuracy: 0.874129593372345\n",
      "Layers: 2, Neurons: 128, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.01, Test Loss: 0.6491703987121582, Test Accuracy: 0.7921799421310425\n",
      "Layers: 2, Neurons: 128, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.1, Test Loss: 3.2792158126831055, Test Accuracy: 0.04043920710682869\n",
      "Layers: 2, Neurons: 128, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.001, Test Loss: 0.5219731330871582, Test Accuracy: 0.8462774753570557\n",
      "Layers: 2, Neurons: 128, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.01, Test Loss: 0.759446382522583, Test Accuracy: 0.7672737240791321\n",
      "Layers: 2, Neurons: 128, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.1, Test Loss: 3.2830164432525635, Test Accuracy: 0.04365291818976402\n",
      "Layers: 2, Neurons: 128, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.001, Test Loss: 0.40586286783218384, Test Accuracy: 0.8770754933357239\n",
      "Layers: 2, Neurons: 128, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.01, Test Loss: 0.5927743315696716, Test Accuracy: 0.8269951939582825\n",
      "Layers: 2, Neurons: 128, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.1, Test Loss: 4.814598083496094, Test Accuracy: 0.1534547358751297\n",
      "Layers: 2, Neurons: 128, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.001, Test Loss: 0.4572545289993286, Test Accuracy: 0.8634172677993774\n",
      "Layers: 2, Neurons: 128, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.01, Test Loss: 0.6456061601638794, Test Accuracy: 0.8015533089637756\n",
      "Layers: 2, Neurons: 128, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.1, Test Loss: 5.644856929779053, Test Accuracy: 0.12372790277004242\n",
      "Layers: 2, Neurons: 128, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.001, Test Loss: 0.5522726774215698, Test Accuracy: 0.8339582085609436\n",
      "Layers: 2, Neurons: 128, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.01, Test Loss: 0.7266846895217896, Test Accuracy: 0.7825388312339783\n",
      "Layers: 2, Neurons: 128, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.1, Test Loss: 6.4121294021606445, Test Accuracy: 0.07391536980867386\n",
      "Layers: 3, Neurons: 32, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.001, Test Loss: 0.79833984375, Test Accuracy: 0.7608462572097778\n",
      "Layers: 3, Neurons: 32, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.01, Test Loss: 0.7943874597549438, Test Accuracy: 0.7587038278579712\n",
      "Layers: 3, Neurons: 32, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.1, Test Loss: 3.276150941848755, Test Accuracy: 0.03829673305153847\n",
      "Layers: 3, Neurons: 32, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.001, Test Loss: 1.0657376050949097, Test Accuracy: 0.6821103096008301\n",
      "Layers: 3, Neurons: 32, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.01, Test Loss: 1.1468626260757446, Test Accuracy: 0.6523835062980652\n",
      "Layers: 3, Neurons: 32, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.1, Test Loss: 3.2771642208099365, Test Accuracy: 0.03883235156536102\n",
      "Layers: 3, Neurons: 32, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.001, Test Loss: 1.380092978477478, Test Accuracy: 0.594001054763794\n",
      "Layers: 3, Neurons: 32, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.01, Test Loss: 1.4230650663375854, Test Accuracy: 0.5680235624313354\n",
      "Layers: 3, Neurons: 32, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.1, Test Loss: 3.2762451171875, Test Accuracy: 0.03910015895962715\n",
      "Layers: 3, Neurons: 32, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.001, Test Loss: 0.8289812803268433, Test Accuracy: 0.7485270500183105\n",
      "Layers: 3, Neurons: 32, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.01, Test Loss: 0.7467043399810791, Test Accuracy: 0.7793251276016235\n",
      "Layers: 3, Neurons: 32, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.1, Test Loss: 3.35788631439209, Test Accuracy: 0.07712908089160919\n",
      "Layers: 3, Neurons: 32, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.001, Test Loss: 0.9425357580184937, Test Accuracy: 0.7206748723983765\n",
      "Layers: 3, Neurons: 32, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.01, Test Loss: 0.8931820392608643, Test Accuracy: 0.7340653538703918\n",
      "Layers: 3, Neurons: 32, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.1, Test Loss: 3.755303382873535, Test Accuracy: 0.04499196633696556\n",
      "Layers: 3, Neurons: 32, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.001, Test Loss: 1.1194887161254883, Test Accuracy: 0.6705945134162903\n",
      "Layers: 3, Neurons: 32, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.01, Test Loss: 1.1590067148208618, Test Accuracy: 0.652651309967041\n",
      "Layers: 3, Neurons: 32, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.1, Test Loss: 3.722383499145508, Test Accuracy: 0.040974825620651245\n",
      "Layers: 3, Neurons: 64, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.001, Test Loss: 0.5238519906997681, Test Accuracy: 0.8331547975540161\n",
      "Layers: 3, Neurons: 64, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.01, Test Loss: 0.7359836101531982, Test Accuracy: 0.7755758166313171\n",
      "Layers: 3, Neurons: 64, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.1, Test Loss: 3.2721097469329834, Test Accuracy: 0.03829673305153847\n",
      "Layers: 3, Neurons: 64, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.001, Test Loss: 0.6960592865943909, Test Accuracy: 0.771826446056366\n",
      "Layers: 3, Neurons: 64, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.01, Test Loss: 0.9069961905479431, Test Accuracy: 0.7169255614280701\n",
      "Layers: 3, Neurons: 64, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.1, Test Loss: 3.2767443656921387, Test Accuracy: 0.04311729967594147\n",
      "Layers: 3, Neurons: 64, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.001, Test Loss: 0.8567589521408081, Test Accuracy: 0.7375468611717224\n",
      "Layers: 3, Neurons: 64, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.01, Test Loss: 1.1395827531814575, Test Accuracy: 0.633101224899292\n",
      "Layers: 3, Neurons: 64, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.1, Test Loss: 3.2853243350982666, Test Accuracy: 0.03936797007918358\n",
      "Layers: 3, Neurons: 64, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.001, Test Loss: 0.5071307420730591, Test Accuracy: 0.8494911789894104\n",
      "Layers: 3, Neurons: 64, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.01, Test Loss: 0.7152032256126404, Test Accuracy: 0.7889662384986877\n",
      "Layers: 3, Neurons: 64, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.1, Test Loss: 4.351118087768555, Test Accuracy: 0.06052490696310997\n",
      "Layers: 3, Neurons: 64, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.001, Test Loss: 0.6799495816230774, Test Accuracy: 0.7849491238594055\n",
      "Layers: 3, Neurons: 64, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.01, Test Loss: 0.8076179623603821, Test Accuracy: 0.7613819241523743\n",
      "Layers: 3, Neurons: 64, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.1, Test Loss: 4.647958278656006, Test Accuracy: 0.06213176250457764\n",
      "Layers: 3, Neurons: 64, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.001, Test Loss: 0.767432451248169, Test Accuracy: 0.7675415277481079\n",
      "Layers: 3, Neurons: 64, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.01, Test Loss: 0.9406675100326538, Test Accuracy: 0.7126405835151672\n",
      "Layers: 3, Neurons: 64, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.1, Test Loss: 4.331901550292969, Test Accuracy: 0.04981253296136856\n",
      "Layers: 3, Neurons: 128, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.001, Test Loss: 0.2973881661891937, Test Accuracy: 0.9043920636177063\n",
      "Layers: 3, Neurons: 128, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.01, Test Loss: 0.722368061542511, Test Accuracy: 0.784145712852478\n",
      "Layers: 3, Neurons: 128, Activation: relu, Dropout Rate: 0.1, Learning Rate: 0.1, Test Loss: 3.289245128631592, Test Accuracy: 0.03910015895962715\n",
      "Layers: 3, Neurons: 128, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.001, Test Loss: 0.38360580801963806, Test Accuracy: 0.8800214529037476\n",
      "Layers: 3, Neurons: 128, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.01, Test Loss: 0.9369832873344421, Test Accuracy: 0.698982298374176\n",
      "Layers: 3, Neurons: 128, Activation: relu, Dropout Rate: 0.2, Learning Rate: 0.1, Test Loss: 3.2883431911468506, Test Accuracy: 0.03561864048242569\n",
      "Layers: 3, Neurons: 128, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.001, Test Loss: 0.5424677729606628, Test Accuracy: 0.8411890864372253\n",
      "Layers: 3, Neurons: 128, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.01, Test Loss: 1.0850435495376587, Test Accuracy: 0.6700589060783386\n",
      "Layers: 3, Neurons: 128, Activation: relu, Dropout Rate: 0.3, Learning Rate: 0.1, Test Loss: 3.266709327697754, Test Accuracy: 0.04177825525403023\n",
      "Layers: 3, Neurons: 128, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.001, Test Loss: 0.320910781621933, Test Accuracy: 0.897696852684021\n",
      "Layers: 3, Neurons: 128, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.01, Test Loss: 0.7839916944503784, Test Accuracy: 0.7603106498718262\n",
      "Layers: 3, Neurons: 128, Activation: tanh, Dropout Rate: 0.1, Learning Rate: 0.1, Test Loss: 6.696040630340576, Test Accuracy: 0.05008034408092499\n",
      "Layers: 3, Neurons: 128, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.001, Test Loss: 0.39231008291244507, Test Accuracy: 0.8773433566093445\n",
      "Layers: 3, Neurons: 128, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.01, Test Loss: 0.878747284412384, Test Accuracy: 0.7193358540534973\n",
      "Layers: 3, Neurons: 128, Activation: tanh, Dropout Rate: 0.2, Learning Rate: 0.1, Test Loss: 6.800544261932373, Test Accuracy: 0.04499196633696556\n",
      "Layers: 3, Neurons: 128, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.001, Test Loss: 0.48057666420936584, Test Accuracy: 0.8596678972244263\n",
      "Layers: 3, Neurons: 128, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.01, Test Loss: 1.140711784362793, Test Accuracy: 0.6561328172683716\n",
      "Layers: 3, Neurons: 128, Activation: tanh, Dropout Rate: 0.3, Learning Rate: 0.1, Test Loss: 7.159122467041016, Test Accuracy: 0.03936797007918358\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to create and compile the model\n",
    "def create_model(layers, neurons, activation, dropout_rate, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation=activation, input_shape=(X_train_imputed.shape[1],)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    for _ in range(layers - 1):\n",
    "        model.add(Dense(neurons, activation=activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(26, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "layers_list = [1, 2, 3]\n",
    "neurons_list = [32, 64, 128]\n",
    "activation_list = ['relu', 'tanh']\n",
    "dropout_rate_list = [0.1, 0.2, 0.3]\n",
    "learning_rate_list = [0.001, 0.01, 0.1]\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over hyperparameters\n",
    "for layers in layers_list:\n",
    "    for neurons in neurons_list:\n",
    "        for activation in activation_list:\n",
    "            for dropout_rate in dropout_rate_list:\n",
    "                for learning_rate in learning_rate_list:\n",
    "                    print(f\"Training model with layers={layers}, neurons={neurons}, activation={activation}, dropout_rate={dropout_rate}, learning_rate={learning_rate}\")\n",
    "\n",
    "                    # Create and compile the model\n",
    "                    model = create_model(layers, neurons, activation, dropout_rate, learning_rate)\n",
    "\n",
    "                    # Train the model\n",
    "                    history = model.fit(X_train_imputed, y_train_encoded, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "                    # Evaluate the model\n",
    "                    test_loss, test_accuracy = model.evaluate(X_test_imputed, y_test_encoded, verbose=0)\n",
    "\n",
    "                    # Store results\n",
    "                    results.append((layers, neurons, activation, dropout_rate, learning_rate, test_loss, test_accuracy))\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(f\"Layers: {result[0]}, Neurons: {result[1]}, Activation: {result[2]}, Dropout Rate: {result[3]}, Learning Rate: {result[4]}, Test Loss: {result[5]}, Test Accuracy: {result[6]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s79lxZ2UCGzM"
   },
   "source": [
    "#Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_mHpWHev0V9x",
    "outputId": "5f22d42f-b4cb-4337-8a89-4ecbbb4823c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388us/step\n",
      "Accuracy: 0.03936797000535619\n",
      "Precision: 0.022658118964020167\n",
      "Recall: 0.03936797000535619\n",
      "F1-score: 0.007521677296925071\n",
      "Confusion Matrix:\n",
      "[[  0   0   0   0   0   0   0   0   0   2   0   4   0   0   0   0 135   0\n",
      "    0   0   0   0   0   0   3   0]\n",
      " [  0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   3 150   0\n",
      "    0   0   0   0   0   0   2   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 140   0\n",
      "    0   0   0   1   0   0   6   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0   1   1   2 156   0\n",
      "    0   0   0   1   0   0   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1 141   0\n",
      "    0   0   0   1   0   0   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   8 134   0\n",
      "    0   0   1   0   0   0   3   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   1   0   0   1   1 165   0\n",
      "    0   0   0   1   0   0   3   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1 126   0\n",
      "    0   0   0   1   0   0   4   0]\n",
      " [  0   0   1   1   0   0   0   0   0   0   0   4   0   0   0   6  94   0\n",
      "    0   0   0   0   0   0   2   0]\n",
      " [  0   0   0   0   0   0   0   0   0   2   0   3   0   1   1   2 132   0\n",
      "    0   0   0   0   2   0   5   2]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2 143   0\n",
      "    0   0   0   0   0   0   4   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   2   0   0   1   2 137   0\n",
      "    0   0   0   0   0   0   4   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   2  11 124   0\n",
      "    0   0   0   0   0   0   6   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   2   0   0   0   2 130   0\n",
      "    0   0   0   2   0   0   1   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0   1   1   1 125   0\n",
      "    0   0   0   1   0   0   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   1   0   0   0   3 152   0\n",
      "    0   0   0   0   0   0   4   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0   0   2   1 136   0\n",
      "    0   0   0   1   0   0   4   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   3 140   0\n",
      "    0   0   0   1   0   0   3   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 138   0\n",
      "    0   0   0   0   0   0   4   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   0   0   1   0   5 142   0\n",
      "    0   0   1   0   0   0   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 141   0\n",
      "    0   0   0   0   0   0   2   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   4 148   0\n",
      "    0   1   0   0   0   0   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0 130   0\n",
      "    0   0   0   0   0   0   2   0]\n",
      " [  0   0   1   0   0   0   1   0   0   1   0   0   0   0   0   1 117   0\n",
      "    0   0   0   1   0   0   2   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   2 131   0\n",
      "    0   0   0   1   0   0   3   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4 115   0\n",
      "    0   0   0   1   0   0   3   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Get the true labels\n",
    "y_true = np.argmax(y_test_encoded, axis=1)\n",
    "\n",
    "# Get the predicted labels\n",
    "y_pred = np.argmax(model.predict(X_test_imputed), axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
